{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fcSX48iBj2Tr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "link = 'https://openlibrary.org/subjects'\n",
        "r = requests.get(link)\n",
        "\n",
        "soup = BeautifulSoup(r.text, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Extracting Topic links"
      ],
      "metadata": {
        "id": "laMlsgOCqYQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = []\n",
        "\n",
        "for i in tqdm(soup.find_all('li')):\n",
        "  try:\n",
        "    link = i.find('a').get('href')\n",
        "    if (len(link.split('/')) == 3):\n",
        "      link = 'https://openlibrary.org/search?subject=' + link.split('/')[-1].lower()\n",
        "\n",
        "      link_ = link.replace('search?subject=','subjects/')\n",
        "      r = requests.get(link_)\n",
        "      soup = BeautifulSoup(r.text, 'html.parser')\n",
        "      pages_ = int(soup.find('span', class_ = 'count').text.strip().split(' ')[0])//20\n",
        "\n",
        "      links.append([link,pages_])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "links = links[6:-5]\n",
        "\n",
        "links = links[6:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIV_gbRUqYhg",
        "outputId": "ffe6b0d0-7978-4495-8243-3bc8e4a9824a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [02:19<00:00,  1.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Scraping Full Data"
      ],
      "metadata": {
        "id": "FnCepKyX8Nih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for link in links:\n",
        "  url = link[0]\n",
        "  pages = link[1]\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for page in tqdm(range(1, pages + 1)):\n",
        "\n",
        "    category = url.split('=')[-1]\n",
        "    response = requests.get(url + '&page=' + str(page))\n",
        "    # time.sleep(2)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    book_elements = soup.find_all(\"li\", {\"class\": \"searchResultItem\"})\n",
        "\n",
        "    for book in book_elements:\n",
        "      try:\n",
        "        try:\n",
        "          title_element = book.find(\"h3\")\n",
        "        except:\n",
        "          title_element = np.nan\n",
        "\n",
        "        try:\n",
        "          title = title_element.text.strip()\n",
        "        except:\n",
        "          title = np.nan\n",
        "\n",
        "        try:\n",
        "          title_id = title_element.find('a').get('href')\n",
        "        except:\n",
        "          title_id = np.nan\n",
        "\n",
        "        try:\n",
        "          author_element = book.find(\"span\", class_ = 'bookauthor').find('a')\n",
        "        except:\n",
        "          author_element\n",
        "\n",
        "        try:\n",
        "          author = author_element.text.strip()\n",
        "        except:\n",
        "          author = np.nan\n",
        "\n",
        "        try:\n",
        "          author_id = author_element.get('href')\n",
        "        except:\n",
        "          author_id = np.nan\n",
        "\n",
        "        try:\n",
        "          publish_year = book.find('span', class_ = 'publishedYear').text.strip()\n",
        "        except:\n",
        "          publish_year = np.nan\n",
        "\n",
        "        try:\n",
        "          cover_url = book.find(\"img\").get('src')\n",
        "        except:\n",
        "          cover_url = np.nan\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      data.append([title, author, category, publish_year, title_id, author_id,cover_url])\n",
        "\n",
        "  df = pd.DataFrame(data, columns = ['title', 'author','category', 'publish_year','title_id','author_id','cover_url'])\n",
        "\n",
        "  df.to_csv(category + '.csv',index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apaamCESsKPw",
        "outputId": "7e7263ee-7401-42b8-abc8-5e47862fe15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 156/3751 [06:24<2:08:09,  2.14s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjLZsOfQiLwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}